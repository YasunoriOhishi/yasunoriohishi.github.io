<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yasunori Ohishi</title>
    <link>https://yasunorio.github.io/</link>
      <atom:link href="https://yasunorio.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yasunori Ohishi</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Yasunori Ohishi</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yasunorio.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Yasunori Ohishi</title>
      <link>https://yasunorio.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://yasunorio.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A paper presented in DCASE Workshop 2021</title>
      <link>https://yasunorio.github.io/post/toyadmos2/</link>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/post/toyadmos2/</guid>
      <description>&lt;p&gt;Please check out our DCASE2021 paper &amp;quot;ToyADMOS2: Another dataset of minauture-machine operating sounds for anomalous sound detection under domain shift conditions&amp;quot; by Noboru Harada, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Masahiro Yasuda, Shoichiro Saito.&lt;/p&gt;
&lt;p&gt;This paper proposes a new large-scale dataset called “ToyADMOS2” for anomaly detection in machine operating sounds (ADMOS). As did for our previous ToyADMOS dataset, we collected a large number of operating sounds of miniature machines (toys) under normal and anomaly conditions by deliberately damaging them but extended with providing controlled depth of damages in anomaly samples. Since typical application scenarios of ADMOS often require robust performance under domain-shift conditions, the ToyADMOS2 dataset is designed for evaluating systems under such conditions. The dataset is freely available for download at &lt;a href=&#34;https://github.com/nttcslab/ToyADMOS2-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/nttcslab/ToyADMOS2-dataset&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.5281/zenodo.4580270&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.5281/zenodo.4580270&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our dataset publicly available</title>
      <link>https://yasunorio.github.io/post/placesjapaneseaudiocaptions/</link>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/post/placesjapaneseaudiocaptions/</guid>
      <description>&lt;p&gt;We are excited to announce that our dataset, The Places audio caption (Japanese) 100K corpus, is now available.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zenodo.org/record/5563425#.YZZKy2DP0UE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zenodo.org/record/5563425#.YZZKy2DP0UE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This speech corpus was collected to investigate the learning of spoken language (words, sub-word units, higher-level semantics, etc.) from visually-grounded speech. For a description of the corpus, see:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Yasunori Ohishi, Akisato Kimura, Takahito Kawanishi, Kunio Kashino, David Harwath, and James Glass,
&amp;ldquo;Trilingual Semantic Embeddings of Visually Grounded Speech with Self-attention Mechanisms,&amp;rdquo; in Proc. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 4352–4356.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The corpus only includes audio recordings, and not the associated images. You will need to separately download the Places image dataset &lt;a href=&#34;http://places.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The data is distributed under &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/legalcode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the Creative Commons Attribution-ShareAlike (CC BY-SA) license&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you use this data in your own publications, please cite the above paper.
This is a collaborative work with &lt;a href=&#34;https://www.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT CSAIL&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ToyADMOS2: Another dataset of minauture-machine operating sounds for anomalous sound detection under domain shift conditions</title>
      <link>https://yasunorio.github.io/publication/2021b/</link>
      <pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2021b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>分散カメラ・分散マイクを利用したイベント検出のためのSelf-Attentionに基づくマルチセンサ統合</title>
      <link>https://yasunorio.github.io/publication/2021c/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2021c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation</title>
      <link>https://yasunorio.github.io/publication/2021a/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2021a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>分散マイク・分散カメラの空間位置情報を活用したマルチモーダルシーン分類</title>
      <link>https://yasunorio.github.io/publication/2021d/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2021d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>対象調波畳み込み</title>
      <link>https://yasunorio.github.io/publication/2021e/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2021e/</guid>
      <description></description>
    </item>
    
    <item>
      <title>音響説明文生成のためのキーワード推定サブタスクの効果</title>
      <link>https://yasunorio.github.io/publication/2021f/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2021f/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsupervised co-segmentation for athlete movements and live commentaries using crossmodal temporal proximity</title>
      <link>https://yasunorio.github.io/publication/2020g/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020g/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval</title>
      <link>https://yasunorio.github.io/publication/2020h/</link>
      <pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020h/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of word-frequency based pre- and post- processings for audio captioning</title>
      <link>https://yasunorio.github.io/publication/2020e/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020e/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pair expansion for learning multilingual semantic embeddings using disjoint visually-grounded speech audio datasets</title>
      <link>https://yasunorio.github.io/publication/2020d/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Crossmodal sound retrieval based on specific target co-occurrence denoted with weak labels</title>
      <link>https://yasunorio.github.io/publication/2020c/</link>
      <pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Harmonic lowering for accelerating harmonic convolution for audio signals</title>
      <link>https://yasunorio.github.io/publication/2020b/</link>
      <pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>弱ラベルで示される特定の共起関係に基づいたクロスモーダル音検索</title>
      <link>https://yasunorio.github.io/publication/2020i/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020i/</guid>
      <description></description>
    </item>
    
    <item>
      <title>調波畳み込みの高速化</title>
      <link>https://yasunorio.github.io/publication/2020j/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020j/</guid>
      <description></description>
    </item>
    
    <item>
      <title>非定常音を含む単変量時系列に対する動的モード分解を用いた特徴量抽出及び可視化の検討</title>
      <link>https://yasunorio.github.io/publication/2020k/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020k/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The NTT DCASE2020 Challenge Task 6 System: Automated audio captioning with keywords and sentence length estimation</title>
      <link>https://yasunorio.github.io/publication/2020f/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020f/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trilingual semantic embeddings of visually grounded speech with self-attention mechanisms</title>
      <link>https://yasunorio.github.io/publication/2020a/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>環境音とラベル情報のデュアルエンコーダに基づくスペクトログラムマスクの生成</title>
      <link>https://yasunorio.github.io/publication/2020l/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2020l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>環境音分析の研究を促進させる競争型ワークショップ</title>
      <link>https://yasunorio.github.io/publication/2019b/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2019b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>画像を説明する多言語音声データを利用したクロスモーダル探索</title>
      <link>https://yasunorio.github.io/publication/2019a/</link>
      <pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2019a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://yasunorio.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ビブリオ・トーク -私のオススメ-：世界の不思議な音　奇妙な音の謎を科学で解き明かす</title>
      <link>https://yasunorio.github.io/publication/2019c/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2019c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Project</title>
      <link>https://yasunorio.github.io/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[Invited] あらゆる音の検出・識別を目指して --音響イベント検出研究の現在と未来--</title>
      <link>https://yasunorio.github.io/publication/2014/</link>
      <pubDate>Fri, 05 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/publication/2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://yasunorio.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yasunorio.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
