[{"authors":null,"categories":null,"content":"Yasunori Ohishi is a senior research scientist of Recognition Research Group at Media Information Laboratory, NTT Communication Science Laboratories. His research interests include acoustic signal processing, multimedia content analysis, and music information retrieval.\n","date":1637193600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1637193600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Yasunori Ohishi is a senior research scientist of Recognition Research Group at Media Information Laboratory, NTT Communication Science Laboratories. His research interests include acoustic signal processing, multimedia content analysis, and music information retrieval.","tags":null,"title":"Yasunori Ohishi","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://yasunorio.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Yasunori Ohishi"],"categories":["Dataset","Anomaly_detection"],"content":"Please check out our DCASE2021 paper \u0026quot;ToyADMOS2: Another dataset of minauture-machine operating sounds for anomalous sound detection under domain shift conditions\u0026quot; by Noboru Harada, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Masahiro Yasuda, Shoichiro Saito.\nThis paper proposes a new large-scale dataset called “ToyADMOS2” for anomaly detection in machine operating sounds (ADMOS). As did for our previous ToyADMOS dataset, we collected a large number of operating sounds of miniature machines (toys) under normal and anomaly conditions by deliberately damaging them but extended with providing controlled depth of damages in anomaly samples. Since typical application scenarios of ADMOS often require robust performance under domain-shift conditions, the ToyADMOS2 dataset is designed for evaluating systems under such conditions. The dataset is freely available for download at https://github.com/nttcslab/ToyADMOS2-dataset and https://doi.org/10.5281/zenodo.4580270.\n","date":1637193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637193600,"objectID":"a5e17252980b08304f9b97b8cad10248","permalink":"https://yasunorio.github.io/post/toyadmos2/","publishdate":"2021-11-18T00:00:00Z","relpermalink":"/post/toyadmos2/","section":"post","summary":"Please check out our DCASE Workshop 2021 paper \"ToyADMOS2: Another dataset of minauture-machine operating sounds for anomalous sound detection under domain shift conditions\" by Noboru Harada, Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Masahiro Yasuda, Shoichiro Saito.","tags":["Dataset","Anomaly_detection"],"title":"A paper presented in DCASE Workshop 2021","type":"post"},{"authors":["Yasunori Ohishi"],"categories":["Dataset","Visually-grounded speech","Crossmodal"],"content":"We are excited to announce that our dataset, The Places audio caption (Japanese) 100K corpus, is now available.\nhttps://zenodo.org/record/5563425#.YZZKy2DP0UE\nThis speech corpus was collected to investigate the learning of spoken language (words, sub-word units, higher-level semantics, etc.) from visually-grounded speech. For a description of the corpus, see:\nYasunori Ohishi, Akisato Kimura, Takahito Kawanishi, Kunio Kashino, David Harwath, and James Glass, \u0026ldquo;Trilingual Semantic Embeddings of Visually Grounded Speech with Self-attention Mechanisms,\u0026rdquo; in Proc. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 4352–4356.\nThe corpus only includes audio recordings, and not the associated images. You will need to separately download the Places image dataset here.\nThe data is distributed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.\nIf you use this data in your own publications, please cite the above paper. This is a collaborative work with MIT CSAIL.\n","date":1637193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637193600,"objectID":"e979075548a736b0fdf98696ee8e5ff8","permalink":"https://yasunorio.github.io/post/placesjapaneseaudiocaptions/","publishdate":"2021-11-18T00:00:00Z","relpermalink":"/post/placesjapaneseaudiocaptions/","section":"post","summary":"We are excited to announce that our dataset, The Places audio caption (Japanese) 100K corpus, is now available. This speech corpus was collected to investigate the learning of spoken language (words, sub-word units, higher-level semantics, etc.) from visually-grounded speech.","tags":["Dataset","Visually-grounded speech","Crossmodal"],"title":"Our dataset publicly available","type":"post"},{"authors":["Noboru Harada","Daisuke Niizumi","Daiki Takeuchi","Yasunori Ohishi","Masahiro Yasuda","Shoichiro Saito"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"378bedff4776fe04294d3d1920c6846d","permalink":"https://yasunorio.github.io/publication/2021b/","publishdate":"2021-10-15T00:00:00Z","relpermalink":"/publication/2021b/","section":"publication","summary":"This paper proposes a new large-scale dataset called \"ToyADMOS2\" for anomaly detection in machine operating sounds (ADMOS). As did for our previous ToyADMOS dataset, we collected a large number of operating sounds of miniature machines (toys) under normal and anomaly conditions by deliberately damaging them but extended with providing controlled depth of damages in anomaly samples. Since typical application scenarios of ADMOS often require robust performance under domain-shift conditions, the ToyADMOS2 dataset is designed for evaluating systems under such conditions. The released dataset consists of two sub-datasets for machine-condition inspection: fault diagnosis of machines with geometrically fixed tasks and fault diagnosis of machines with moving tasks. Domain shifts are represented by introducing several differences in operating conditions, such as the use of the same machine type but with different machine models and parts configurations, different operating speeds, microphone arrangements, etc. Each sub-dataset contains over 27 k samples of normal machineoperating sounds and over 8 k samples of anomalous sounds recorded with five to eight microphones. The dataset is freely available for download at https://github.com/nttcslab/ToyADMOS2-dataset and https://doi.org/10.5281/zenodo.4580270.","tags":[],"title":"ToyADMOS2: Another dataset of minauture-machine operating sounds for anomalous sound detection under domain shift conditions","type":"publication"},{"authors":["Masahiro Yasuda","Yasunori Ohishi","Shoichiro Saito","Noboru Harada"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631145600,"objectID":"46ccc13630a8be2a679e419e9535c946","permalink":"https://yasunorio.github.io/publication/2021c/","publishdate":"2021-09-09T00:00:00Z","relpermalink":"/publication/2021c/","section":"publication","summary":"","tags":[],"title":"分散カメラ・分散マイクを利用したイベント検出のためのSelf-Attentionに基づくマルチセンサ統合","type":"publication"},{"authors":["Daisuke Niizumi","Daiki Takeuchi","Yasunori Ohishi","Noboru Harada","Kunio Kashino"],"categories":null,"content":"","date":1626566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626566400,"objectID":"79497dfb7ac5b0e6736ecb6f09c4b081","permalink":"https://yasunorio.github.io/publication/2021a/","publishdate":"2021-07-18T00:00:00Z","relpermalink":"/publication/2021a/","section":"publication","summary":"Inspired by the recent progress in self-supervised learning for computer vision that generates supervision using data augmentations, we explore a new general-purpose audio representation learning approach. We propose learning generalpurpose audio representation from a single audio segment without expecting relationships between different time segments of audio samples. To implement this principle, we introduce Bootstrap Your Own Latent (BYOL) for Audio (BYOL-A, pronounced \"viola\"), an audio self-supervised learning method based on BYOL for learning general-purpose audio representation. Unlike most previous audio self-supervised learning methods that rely on agreement of vicinity audio segments or disagreement of remote ones, BYOL-A creates contrasts in an augmented audio segment pair derived from a single audio segment. With a combination of normalization and augmentation techniques, BYOL-A achieves state-of-the-art results in various downstream tasks. Extensive ablation studies also clarified the contribution of each component and their combinations.","tags":[],"title":"BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation","type":"publication"},{"authors":["Masahiro Yasuda","Yasunori Ohishi","Shoichiro Saito","Yuma Koizumi"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"9b6e5023818e013147e0fd1a66796e1e","permalink":"https://yasunorio.github.io/publication/2021d/","publishdate":"2021-03-12T00:00:00Z","relpermalink":"/publication/2021d/","section":"publication","summary":"","tags":[],"title":"分散マイク・分散カメラの空間位置情報を活用したマルチモーダルシーン分類","type":"publication"},{"authors":["Hirotoshi Takeuchi","Yasunori Ohishi","Kunio Kashino","Hiroshi Saruwatari"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"9cfd216287d85c0225d9ea456fe06535","permalink":"https://yasunorio.github.io/publication/2021e/","publishdate":"2021-03-12T00:00:00Z","relpermalink":"/publication/2021e/","section":"publication","summary":"","tags":[],"title":"対象調波畳み込み","type":"publication"},{"authors":["Daiki Takeuchi","Yasunori Ohishi","Yuma Koizumi","Daisuke Niizumi","Noboru Harada","Kunio Kashino"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"1e6b9b1fde3745612cb62f61c272a4a6","permalink":"https://yasunorio.github.io/publication/2021f/","publishdate":"2021-03-12T00:00:00Z","relpermalink":"/publication/2021f/","section":"publication","summary":"","tags":[],"title":"音響説明文生成のためのキーワード推定サブタスクの効果","type":"publication"},{"authors":["Yasunori Ohishi","Yuki Tanaka","Kunio Kashino"],"categories":null,"content":"","date":1610668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610668800,"objectID":"f1c9da633e4871cd2ec5c76159c668f5","permalink":"https://yasunorio.github.io/publication/2020g/","publishdate":"2021-01-15T00:00:00Z","relpermalink":"/publication/2020g/","section":"publication","summary":"Audio-visual co-segmentation is a task to extract segments and regions corresponding to specific events on unlabeled audio and video signals. It is particularly important to accomplish it in an unsupervised way, since it is generally very difficult to manually label all the objects and events appearing in audio-visual signals for supervised learning. Here, we propose to take advantage of the temporal proximity of corresponding audio and video entities included in the signals. For this purpose, we newly employ a guided attention scheme to this task to efficiently detect and utilize temporal co-occurrences of audio and video information. Experiments using a real TV broadcasts of sumo wrestling, a sport event, with live commentaries show that our model can automatically extract specific athlete movements and its spoken descriptions in an unsupervised manner.","tags":[],"title":"Unsupervised co-segmentation for athlete movements and live commentaries using crossmodal temporal proximity","type":"publication"},{"authors":["Yuma Koizumi","Yasunori Ohishi","Daisuke Niizumi","Daiki Takeuchi","Masahiro Yasuda"],"categories":null,"content":"","date":1607904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607904000,"objectID":"690d3db9e1eb04d77851ac165dc6b786","permalink":"https://yasunorio.github.io/publication/2020h/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/publication/2020h/","section":"publication","summary":"The goal of audio captioning is to translate input audio into its description using natural language. One of the problems in audio captioning is the lack of training data due to the difficulty in collecting audio-caption pairs by crawling the web. In this study, to overcome this problem, we propose to use a pre-trained large-scale language model. Since an audio input cannot be directly inputted into such a language model, we utilize guidance captions retrieved from a training dataset based on similarities that may exist in different audio. Then, the caption of the audio input is generated by using a pretrained language model while referring to the guidance captions. Experimental results show that (i) the proposed method has succeeded to use a pre-trained language model for audio captioning, and (ii) the oracle performance of the pre-trained model-based caption generator was clearly better than that of the conventional method trained from scratch.","tags":[],"title":"Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval","type":"publication"},{"authors":["Daiki Takeuchi","Yuma Koizumi","Yasunori Ohishi","Noboru Harada","Kunio Kashino"],"categories":null,"content":"","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604361600,"objectID":"e8d990215beb6a0da1b8bbb652217dbf","permalink":"https://yasunorio.github.io/publication/2020e/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/publication/2020e/","section":"publication","summary":"The system we used for Task 6 (Automated Audio Captioning) of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge combines three elements, namely, data augmentation, multi-task learning, and post-processing, for audio captioning. The system received the highest evaluation scores, but which of the individual elements most fully contributed to its performance has not yet been clarified. Here, to asses their contributions, we first conducted an element-wise ablation study on our system to estimate to what extent each element is effective. We then conducted a detailed module-wise ablation study to further clarify the key processing modules for improving accuracy. The results show that data augmentation and post-processing significantly improve the score in our system. In particular, mix-up data augmentation and beam search in post-processing improve SPIDEr by 0.8 and 1.6 points, respectively.","tags":[],"title":"Effects of word-frequency based pre- and post- processings for audio captioning","type":"publication"},{"authors":["Yasunori Ohishi","Akisato Kimura","Takahito Kawanishi","Kunio Kashino","David Harwath","James Glass"],"categories":null,"content":"","date":1603756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603756800,"objectID":"50dd133dab2e07461227c7174f25405f","permalink":"https://yasunorio.github.io/publication/2020d/","publishdate":"2020-10-27T00:00:00Z","relpermalink":"/publication/2020d/","section":"publication","summary":"We propose a data expansion method for learning a multilingual semantic embedding model using disjoint datasets containing images and their multilingual audio captions. Here, disjoint means that there are no shared images among the multiple language datasets, in contrast to existing works on multilingual semantic embedding based on visually-grounded speech audio, where it has been assumed that each image is associated with spoken captions of multiple languages. Although learning on disjoint datasets is more challenging, we consider it crucial in practical situations. Our main idea is to refer to another paired data when evaluating a loss value regarding an anchor image. We call this scheme “pair expansion”. The motivation behind this idea is to utilize even disjoint pairs by finding similarities, or commonalities, that may exist in different images. Specifically, we examine two approaches for calculating similarities: one using image embedding vectors and the other using object recognition results. Our experiments show that expanded pairs improve crossmodal and cross-lingual retrieval accuracy compared with non-expanded cases. They also show that similarities measured by the image embedding vectors yield better accuracy than those based on object recognition results.","tags":[],"title":"Pair expansion for learning multilingual semantic embeddings using disjoint visually-grounded speech audio datasets","type":"publication"},{"authors":["Masahiro Yasuda","Yasunori Ohishi","Yuma Koizumi","Noboru Harada"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"2c25f93bd5d79bd1d81575160538dd26","permalink":"https://yasunorio.github.io/publication/2020c/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/publication/2020c/","section":"publication","summary":"Recent advancements in representation learning enable cross-modal retrieval by modeling an audio-visual co-occurrence in a single aspect, such as physical and linguistic. Unfortunately, in real-world media data, since co-occurrences in various aspects are complexly mixed, it is difficult to distinguish a specific target co-occurrence from many other non-target co-occurrences, resulting in failure in crossmodal retrieval. To overcome this problem, we propose a triplet-loss-based representation learning method that incorporates an awareness mechanism. We adopt weakly-supervised event detection, which provides a constraint in representation learning so that our method can “be aware” of a specific target audio-visual co-occurrence and discriminate it from other non-target co-occurrences. We evaluated the performance of our method by applying it to a sound effect retrieval task using recorded TV broadcast data. In the task, a sound effect appropriate for a given video input should be retrieved. We then conducted objective and subjective evaluations, the results indicating that the proposed method produces significantly better associations of sound and visual effects than baselines with no awareness mechanism.","tags":[],"title":"Crossmodal sound retrieval based on specific target co-occurrence denoted with weak labels","type":"publication"},{"authors":["Hirotoshi Takeuchi","Kunio Kashino","Yasunori Ohishi","Hiroshi Saruwatari"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"301a001bed99ddd0817b5ec1757eb4f9","permalink":"https://yasunorio.github.io/publication/2020b/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/publication/2020b/","section":"publication","summary":"Convolutional neural networks have been successfully applied to a variety of audio signal processing tasks including sound source separation, speech recognition and acoustic scene understanding. Since many pitched sounds have a harmonic structure, an operation, called harmonic convolution, has been proposed to take advantages of the structure appearing in the audio signals. However, the computational cost involved is higher than that of normal convolution. This paper proposes a faster calculation method of harmonic convolution called Harmonic Lowering. The method unrolls the input data to a redundant layout so that the normal convolution operation can be applied. The analysis of the runtimes and the number of multiplication operations show that the proposed method accelerates the harmonic convolution 2 to 7 times faster than the conventional method under realistic parameter settings, while no approximation is introduced.","tags":[],"title":"Harmonic lowering for accelerating harmonic convolution for audio signals","type":"publication"},{"authors":["Masahiro Yasuda","Yasunori Ohishi","Yuma Koizumi","Noboru Harada"],"categories":null,"content":"","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"c9f059ca48b4e10b32d57e1d6b71724f","permalink":"https://yasunorio.github.io/publication/2020i/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/publication/2020i/","section":"publication","summary":"","tags":[],"title":"弱ラベルで示される特定の共起関係に基づいたクロスモーダル音検索","type":"publication"},{"authors":["Hirotoshi Takeuchi","Kunio Kashino","Yasunori Ohishi","Hiroshi Saruwatari"],"categories":null,"content":"","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"a57ae57e45934eb46d7050e708e67181","permalink":"https://yasunorio.github.io/publication/2020j/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/publication/2020j/","section":"publication","summary":"","tags":[],"title":"調波畳み込みの高速化","type":"publication"},{"authors":["Hiroaki Ito","Shin Murata","Yasunori Ohishi","Noboru Harada"],"categories":null,"content":"","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"9729b99926ab3f6f0b3ef66059aedd21","permalink":"https://yasunorio.github.io/publication/2020k/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/publication/2020k/","section":"publication","summary":"","tags":[],"title":"非定常音を含む単変量時系列に対する動的モード分解を用いた特徴量抽出及び可視化の検討","type":"publication"},{"authors":["Yuma Koizumi","Daiki Takeuchi","Yasunori Ohishi","Noboru Harada","Kunio Kashino"],"categories":null,"content":"","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591142400,"objectID":"d2091bfdb38950b9b3f14ce606e6f26f","permalink":"https://yasunorio.github.io/publication/2020f/","publishdate":"2020-06-03T00:00:00Z","relpermalink":"/publication/2020f/","section":"publication","summary":"This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.","tags":[],"title":"The NTT DCASE2020 Challenge Task 6 System: Automated audio captioning with keywords and sentence length estimation","type":"publication"},{"authors":["Yasunori Ohishi","Akisato Kimura","Takahito Kawanishi","Kunio Kashino","David Harwath","James Glass"],"categories":null,"content":"","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"76195a19bcd5fe72450ffd93e488b981","permalink":"https://yasunorio.github.io/publication/2020a/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/publication/2020a/","section":"publication","summary":"We propose a trilingual semantic embedding model that relates visual objects in an image to a section of audio signal corresponding to spoken words in an unsupervised manner. Unlike existing models, the proposed model incorporates three different languages, namely, English, Hindi, and Japanese. To build the model, we newly collect a Japanese speech caption data set, in addition to existing English and Hindi sets. We show that introduction of the third language improves the performance on average, in terms of crossmodal and cross-lingual information retrieval accuracy, and that the self-attention mechanisms added to the speech encoders also effectively works.","tags":[],"title":"Trilingual semantic embeddings of visually grounded speech with self-attention mechanisms","type":"publication"},{"authors":["Hirotoshi Takeuchi","Yasunori Ohishi","Takahito Kawanishi","Kunio Kashino"],"categories":null,"content":"","date":1584403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584403200,"objectID":"7ab30e41b4dfed43782a9b9598f89a9c","permalink":"https://yasunorio.github.io/publication/2020l/","publishdate":"2020-03-17T00:00:00Z","relpermalink":"/publication/2020l/","section":"publication","summary":"","tags":[],"title":"環境音とラベル情報のデュアルエンコーダに基づくスペクトログラムマスクの生成","type":"publication"},{"authors":["Yasunori Ohishi"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"b91143a0df22daa3ca8e1a6011637a92","permalink":"https://yasunorio.github.io/publication/2019b/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/2019b/","section":"publication","summary":"","tags":[],"title":"環境音分析の研究を促進させる競争型ワークショップ","type":"publication"},{"authors":["Yasunori Ohishi","Akisato Kimura","Takahito Kawanishi","Kunio Kashino","David Harwath","James Glass"],"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559260800,"objectID":"50b3d9dd4429b0077bb39fdbd11c05cb","permalink":"https://yasunorio.github.io/publication/2019a/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/publication/2019a/","section":"publication","summary":"","tags":[],"title":"画像を説明する多言語音声データを利用したクロスモーダル探索","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yasunorio.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Yasunori Ohishi"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"47dd39c78fda4421b2a17261d8201f66","permalink":"https://yasunorio.github.io/publication/2019c/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/2019c/","section":"publication","summary":"","tags":[],"title":"ビブリオ・トーク -私のオススメ-：世界の不思議な音　奇妙な音の謎を科学で解き明かす","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yasunorio.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Yasunori Ohishi"],"categories":null,"content":"","date":1409875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409875200,"objectID":"7bd64e0de0683688983d85c42fdb1d11","permalink":"https://yasunorio.github.io/publication/2014/","publishdate":"2014-09-05T00:00:00Z","relpermalink":"/publication/2014/","section":"publication","summary":"","tags":[],"title":"[Invited] あらゆる音の検出・識別を目指して --音響イベント検出研究の現在と未来--","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://yasunorio.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]